{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gusanagy/DeepLabV3/blob/main/DeeplabV3%2B_Autoceres.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfXEOh7fE3Mb"
      },
      "source": [
        "#DeeplabV3+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hv4-jio5yR7"
      },
      "source": [
        "1. Organizar codigo\n",
        "\n",
        "\n",
        "2. Funcaoes de melhoria\n",
        "  * Adicionar funcao que salva as predições  (*Checkpoint*)\n",
        "  * Adicionar func que carrega as predições (*Load Checkpoint*)\n",
        "  * adicionar funcoes de augumentation\n",
        "\n",
        "3. Funcoes de avaliacao\n",
        "  * investigar tensor boards para avaliações\n",
        "  * Ajustar metricas de avaliação\n",
        "  Por slgum moptivo os calculos das metricas nao estao corretos\n",
        "\n",
        "4. Erros remanescentes\n",
        "  * investigar normalizacao\n",
        "  * Mudar tamanho padrão das imagens\n",
        "\n",
        "5. Atualizar no github\n",
        "\n",
        "6. Quanto aos canais de cor na classificacao binaria\n",
        " * A comparacao da predicao e da label nesse caso compara tres canais de cores com dois canais numa classicificação binaria. Como interpretar este fato. Afinal para calcular acuracia os dados devem tero mesmo formato.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk5t2GCtGZ1o"
      },
      "source": [
        "##Bibliotecas usadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy_7V0RnvhjX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "#from albumentations import Compose, HorizontalFlip, ChannelShuffle, Resize, CoarseDropout, Rotate, Equalize, RandomShadow, CenterCrop, RandomBrightnessContrast, HorizontalFlip, CLAHE, Normalize\n",
        "#from albumentations.augmentations.geometric.resize import Resize as\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import albumentations as A\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch import nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Resize\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms import ConvertImageDtype\n",
        "from torchvision import datasets, transforms\n",
        "#from torchvision.transforms import Compose\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from timeit import default_timer as timer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6ahOXqrsO20"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR7odsiNoZ-E",
        "outputId": "80a0e1b3-e2a5-40b0-d877-b7b7968b5e13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "helper_functions.py already exists, skipping download\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  # Note: you need the \"raw\" GitHub URL for this to work\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Fr_zuauo4tM"
      },
      "outputs": [],
      "source": [
        "# Import accuracy metric\n",
        "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
        "\n",
        "def print_train_time(start: float,\n",
        "                     end: float,\n",
        "                     device: torch.device = None):\n",
        "  \"\"\"Prints difference between start and end time. \"\"\"\n",
        "  total_time = end-start\n",
        "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "  return total_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-UGizAx5nDC"
      },
      "source": [
        "### Wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ylfcl-c5pEW",
        "outputId": "ab4a3ce9-1c8c-4c2c-a59e-8d64f87b7073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.12)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.37)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.31.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPGZlOHQ5s0R",
        "outputId": "4eaa8d2b-eb03-42ac-ccea-b92539178c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxz8nbCd571p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248,
          "referenced_widgets": [
            "ff4989e6af294d5da79db20d0f39af97"
          ]
        },
        "outputId": "36e997a1-c84a-4414-9d6c-7c7244d32682"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:iwe16z7p) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff4989e6af294d5da79db20d0f39af97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">noble-serenity-9</strong> at: <a href='https://wandb.ai/autoceres/DeepLabV3/runs/iwe16z7p' target=\"_blank\">https://wandb.ai/autoceres/DeepLabV3/runs/iwe16z7p</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>/content/wandb/run-20231004_174838-iwe16z7p/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:iwe16z7p). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231004_182553-xg6c5lnh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/autoceres/DeepLabV3/runs/xg6c5lnh' target=\"_blank\">vital-glitter-10</a></strong> to <a href='https://wandb.ai/autoceres/DeepLabV3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/autoceres/DeepLabV3' target=\"_blank\">https://wandb.ai/autoceres/DeepLabV3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/autoceres/DeepLabV3/runs/xg6c5lnh' target=\"_blank\">https://wandb.ai/autoceres/DeepLabV3/runs/xg6c5lnh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/autoceres/DeepLabV3/runs/xg6c5lnh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c490c9b1f90>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "wandb.init(\n",
        "  project=\"DeepLabV3\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qus1XC8FHJJF"
      },
      "source": [
        "## Montar Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z2IwuLgsNVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f888cc-1c05-4c07-8e00-28d52e3bca0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n",
            "checkpoints  .gitignore\t\t  main.py     __pycache__\tutils\n",
            "dataset      Heart.csv\t\t  metrics     README.md\t\twandb\n",
            "datasets     helper_functions.py  network     requirements.txt\n",
            ".git\t     LICENSE\t\t  predict.py  samples\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# montar Google Drive / mount Google drive\n",
        "# será solicitadas as credenciais do usuario\n",
        "# do gmail\n",
        "drive.mount('/content/gdrive/',force_remount=True)\n",
        "#ir para pasta conhecimento\n",
        "path = '/content/gdrive/MyDrive/DeepLabV3Plus-Pytorch/'\n",
        "#ir para pasta conhecimento\n",
        "os.chdir(path)\n",
        "\n",
        "#listar arquivos da pasta\n",
        "!ls -A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Feeg3yHGGe5h"
      },
      "source": [
        "##Data Augmentation extra // Não rodar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY0SPQFmR5A6"
      },
      "outputs": [],
      "source": [
        "\"\"\" Diretório \"\"\"\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def load_data(path, split = 0.25):\n",
        "    \"\"\" Carregamento das imagens e máscaras \"\"\"\n",
        "    X = sorted(glob(os.path.join(path, \"/content/gdrive/MyDrive/DeepLabV3Plus-Pytorch/datasets/Dataset_Strawberry/images\", \"*.png\")))\n",
        "    Y = sorted(glob(os.path.join(path, \"/content/gdrive/MyDrive/DeepLabV3Plus-Pytorch/datasets/Dataset_Strawberry/masks\", \"*.png\")))\n",
        "\n",
        "    split_size = int(len(X) * split)\n",
        "\n",
        "    train_x, test_x = train_test_split(X, test_size = split_size, random_state = 42)\n",
        "    train_y, test_y = train_test_split(Y, test_size = split_size, random_state = 42)\n",
        "\n",
        "    return (train_x, train_y), (test_x, test_y)\n",
        "def augment_data(images, masks, save_path, augment = True):\n",
        "    H = 512\n",
        "    W = 512\n",
        "\n",
        "    for x, y in tqdm(zip(images, masks), total = len(images)):\n",
        "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "        x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "        y = cv2.imread(y, cv2.IMREAD_COLOR)\n",
        "\n",
        "        \"\"\" Data Augmentation \"\"\"\n",
        "        if augment == True:\n",
        "            aug = HorizontalFlip(p = 1.0) #Flip horizontal\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x1 = augmented[\"image\"]\n",
        "            y1 = augmented[\"mask\"]\n",
        "\n",
        "            x2 = cv2.cvtColor(x, cv2.COLOR_RGB2GRAY)\n",
        "            y2 = y\n",
        "\n",
        "            \"\"\"\n",
        "            aug = ChannelShuffle(p = 1) #Reorganiza aleatoriamente os canais da imagem RGB de entrada.\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x3 = augmented['image']\n",
        "            y3 = augmented['mask']\n",
        "            \"\"\"\n",
        "\n",
        "            aug = CoarseDropout(p = 1, min_holes = 3, max_holes = 10, max_height = 32, max_width = 32) #Dropa uma caixa de pixels 32x32 da imagem, pode simular possíveis falhas na linha de plantação\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x4 = augmented['image']\n",
        "            y4 = augmented['mask']\n",
        "\n",
        "            \"\"\"\n",
        "            aug = Rotate(limit = 45, p = 1.0) #Rotaciona a imagem de entrada\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x5 = augmented[\"image\"]\n",
        "            y5 = augmented[\"mask\"]\n",
        "            \"\"\"\n",
        "\n",
        "            aug = Equalize(p = 1.0) #Equaliza a imagem de entrada\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x5 = augmented[\"image\"]\n",
        "            y5 = augmented[\"mask\"]\n",
        "\n",
        "            \"\"\"\n",
        "            aug = RandomShadow(shadow_roi = (0, 0.5, 1, 1), num_shadows_lower = 1, num_shadows_upper = 1, shadow_dimension = 4, always_apply = False, p=1) #Cria sombras nas imagens\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x6 = augmented[\"image\"]\n",
        "            y6 = augmented[\"mask\"]\n",
        "            \"\"\"\n",
        "\n",
        "            aug = RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1.5) #Brilho aleatório\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x6 = augmented[\"image\"]\n",
        "            y6 = augmented[\"mask\"]\n",
        "\n",
        "            X = [x, x1, x2, x4, x5, x6]\n",
        "            Y = [y, y1, y2, y4, y5, y6]\n",
        "\n",
        "        else:\n",
        "            X = [x]\n",
        "            Y = [y]\n",
        "\n",
        "        index = 0\n",
        "        for i, m in zip(X, Y):\n",
        "            try:\n",
        "                aug = CenterCrop(H, W, p = 1.0)\n",
        "                augmented = aug(image = i, mask = m)\n",
        "                i = augmented[\"image\"]\n",
        "                m = augmented[\"mask\"]\n",
        "\n",
        "            except Exception as e:\n",
        "                i = cv2.resize(i, (W, H))\n",
        "                m = cv2.resize(m, (W, H))\n",
        "\n",
        "            tmp_image_name = f\"{name}_{index}.png\"\n",
        "            tmp_mask_name = f\"{name}_{index}.png\"\n",
        "\n",
        "            image_path = os.path.join(save_path, \"image\", tmp_image_name)\n",
        "            mask_path = os.path.join(save_path, \"mask\", tmp_mask_name)\n",
        "\n",
        "            cv2.imwrite(image_path, i)\n",
        "            cv2.imwrite(mask_path, m)\n",
        "\n",
        "            index += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgyuakE5YJcp"
      },
      "source": [
        "### Data Augmentation Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkdQnNGE5UUH",
        "outputId": "9a3af012-cce9-456e-85de-659ea0312d20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:\t 327 - 327\n",
            "Test:\t 109 - 109\n"
          ]
        }
      ],
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     np.random.seed(42)\n",
        "\n",
        "#     \"\"\" Carregamento do Dataset \"\"\"\n",
        "#     data_path = \"datasets\"\n",
        "\n",
        "#     (train_x, train_y), (test_x, test_y) = load_data(data_path)\n",
        "\n",
        "#     print(f\"Train:\\t {len(train_x)} - {len(train_y)}\")\n",
        "#     print(f\"Test:\\t {len(test_x)} - {len(test_y)}\")\n",
        "\n",
        "    # \"\"\" Criando os diretórios para o Data Augmentation \"\"\"\n",
        "    # create_dir(\"/dataset/new_data/train/image/\")\n",
        "    # create_dir(\"/dataset/new_data/train/mask/\")\n",
        "    # create_dir(\"/dataset/new_data/test/image/\")\n",
        "    # create_dir(\"/dataset/new_data/test/mask/\")\n",
        "\n",
        "    # augment_data(train_x, train_y, \"/dataset/new_data/train/\", augment = True)\n",
        "    # augment_data(test_x, test_y, \"/dataset/new_data/test/\", augment = True)#Substitui por True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhfLNhJlYOkw"
      },
      "source": [
        "## Setup DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoxLNZEBdACX"
      },
      "source": [
        "### Endereço para o dataset Augumentado com a lib augumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f9OCzjNjX7S",
        "outputId": "6ea35431-cd0e-4564-fd26-1f90fa663896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "          Dir of X / no aug:     datasets/Dataset_Strawberry/images/\n",
            "          Dir of y / no aug:     datasets/Dataset_Strawberry/masks/\n",
            "          Dir train x:  dataset/new_data/train/image/\n",
            "          Dir train y:  dataset/new_data/train/mask/\n",
            "          Dir test x:   dataset/new_data/test/image/\n",
            "          Dir test y:   dataset/new_data/test/mask/\n"
          ]
        }
      ],
      "source": [
        "#diretório das imagens de treino\n",
        "TRAIN_IMG_DIR = \"dataset/new_data/train/image/\"\n",
        "#diretório das máscaras de treino\n",
        "TRAIN_MASK_DIR = \"dataset/new_data/train/mask/\"\n",
        "#diretório das imagens de teste\n",
        "VAL_IMG_DIR = \"dataset/new_data/test/image/\"\n",
        "#diretório das máscaras de teste\n",
        "VAL_MASK_DIR = \"dataset/new_data/test/mask/\"\n",
        "\n",
        "#Caso fosse utilizar o dataset sem augumentar\n",
        "DIR_X = \"datasets/Dataset_Strawberry/images/\"\n",
        "DIR_y = \"datasets/Dataset_Strawberry/masks/\"\n",
        "\n",
        "print(f\"\"\"\n",
        "          Dir of X / no aug:     {DIR_X}\n",
        "          Dir of y / no aug:     {DIR_y}\n",
        "          Dir train x:  {TRAIN_IMG_DIR}\n",
        "          Dir train y:  {TRAIN_MASK_DIR}\n",
        "          Dir test x:   {VAL_IMG_DIR}\n",
        "          Dir test y:   {VAL_MASK_DIR}\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZuUivU9dPOo"
      },
      "source": [
        "### Classe Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQzvtBuuzx5v"
      },
      "outputs": [],
      "source": [
        "class AgroDataset(Dataset):\n",
        "    def __init__(self, img_dir_x, img_dir_y, transform=None, transform_2=None):\n",
        "        self.img_dir_x = img_dir_x\n",
        "        self.img_dir_y = img_dir_y\n",
        "        self.train_transform = transform\n",
        "        self.train_transform_2 = transform_2\n",
        "        self.images = os.listdir(img_dir_x)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_dir_x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path_x = os.path.join(self.img_dir_x,self.images[idx].replace(\".jpg\", \".png\"))\n",
        "        img_path_y = os.path.join(self.img_dir_y,self.images[idx].replace(\".jpg\", \".png\"))\n",
        "        image_x =  np.array(Image.open(img_path_x).convert(\"RGB\"))#Modifique para nao carretgar imagens rgb\n",
        "        label_mask_y = np.array(Image.open(img_path_y).convert(\"L\"))\n",
        "\n",
        "        #label_mask_y = label_mask_y.repeat(3, 1, 1)\n",
        "        #image_x = read_image(img_path_x)#.numpy().dtype(np.int32)\n",
        "        #label_mask_y = read_image(img_path_y)#.numpy().dtype(np.int32)\n",
        "        #label_mask_y = Image.open(img_path_y)\n",
        "        #image_x = Image.open(img_path_x)\n",
        "        #label = os.path.basename(X_train[idx])\n",
        "        label_mask_y[label_mask_y == 255.0] = 1.0\n",
        "\n",
        "        #convert_to_binary = lambda x: 1.0 if x == 255 else 0.0\n",
        "        #resultado = list(map(convert_to_binary, label_mask_y))\n",
        "\n",
        "        if self.train_transform is not None:\n",
        "          image_x = self.train_transform(image_x)\n",
        "          label_mask_y = self.train_transform_2(label_mask_y)\n",
        "        return image_x, label_mask_y\n",
        "    # classmethod\n",
        "    # def decode_target(cls, mask):\n",
        "    #     \"\"\"decode semantic mask to RGB image\"\"\"\n",
        "    #     return cls.cmap[mask]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aoqaPRAtkPN"
      },
      "outputs": [],
      "source": [
        "# def one_hot_encode(label, label_values):\n",
        "#     \"\"\"\n",
        "#     Convert a segmentation image label array to one-hot format\n",
        "#     by replacing each pixel value with a vector of length num_classes\n",
        "#     # Arguments\n",
        "#         label: The 2D array segmentation image label\n",
        "#         label_values\n",
        "\n",
        "#     # Returns\n",
        "#         A 2D array with the same width and hieght as the input, but\n",
        "#         with a depth size of num_classes\n",
        "#     \"\"\"\n",
        "#     semantic_map = []\n",
        "#     for colour in label_values:\n",
        "#         equality = np.equal(label, colour)\n",
        "#         class_map = np.all(equality, axis = -1)\n",
        "#         semantic_map.append(class_map)\n",
        "#     semantic_map = np.stack(semantic_map, axis=-1)\n",
        "\n",
        "#     return semantic_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvFlwdZedUIJ"
      },
      "source": [
        "###Instancia a Classe e carrega os dados do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcqvS_rzacmp"
      },
      "outputs": [],
      "source": [
        "def Get_loaders(\n",
        "    TRAIN_IMG_DIR,\n",
        "    TRAIN_MASK_DIR,\n",
        "    VAL_IMG_DIR,\n",
        "    VAL_MASK_DIR,\n",
        "    BATCH_SIZE=8,\n",
        "    NUM_WORKERS=4,\n",
        "    PIN_MEMORY=True,\n",
        "    IMAGE_HEIGHT = 160,\n",
        "    IMAGE_WIDTH = 240):\n",
        "\n",
        "  train_transform = A.Compose(\n",
        "        [\n",
        "            A.Resize(height= IMAGE_HEIGHT,width=IMAGE_WIDTH),\n",
        "            A.Rotate(limit=35, p=0.3),                        #Rotaciona em até 35 graus\n",
        "            A.HorizontalFlip(p=1.0),                          #Espelha horizontalmente com probabilidade de 0,5\n",
        "            A.CLAHE(p=0.2),                                   #ajuda a melhorar a visibilidade de detalhes em regiões escuras ou claras da imagem.\n",
        "            A.Normalize(                   #normaliza os valores de pixel da imagem para ter média 0 e desvio padrão 1\n",
        "                mean=[0.5, 0.5, 0.5],\n",
        "                std=[0.5, 0.5, 0.5],\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2(),     #converte a imagem para o formato tensor\n",
        "        ],\n",
        "    )\n",
        "\n",
        "  test_transform = Compose(\n",
        "        [\n",
        "            A.Resize(height= IMAGE_HEIGHT,width=IMAGE_WIDTH),   #redimensiona a imagem\n",
        "            A.Normalize(\n",
        "                mean=[0.5, 0.5, 0.5],\n",
        "                std=[0.5, 0.5, 0.5],\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2(),\n",
        "        ],\n",
        "    )\n",
        "  # # Definir transformações para o DataLoader\n",
        "  # train_transform = transforms.Compose(\n",
        "  #                               [\n",
        "  #                               transforms.ToPILImage(),\n",
        "  #                               transforms.Resize((IMAGE_WIDTH,IMAGE_HEIGHT)),\n",
        "  #                               transforms.ToTensor(),\n",
        "  #                               #transforms.Lambda(lambda x: torch.cat([x, x, x], dim=0)),\n",
        "  #                               #transforms.Lambda(lambda x: 1.0 if x == 255.0 else x / 255.0),\n",
        "  #                               transforms.ConvertImageDtype(torch.float),\n",
        "  #                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "  #                               #transforms.Normalize((0.485), (0.229)),\n",
        "  #                               ]\n",
        "  # )\n",
        "  # test_transform = transforms.Compose(\n",
        "  #                               [\n",
        "  #                               transforms.ToPILImage(),\n",
        "  #                               transforms.Resize((IMAGE_WIDTH,IMAGE_HEIGHT)),\n",
        "  #                               transforms.ToTensor(),\n",
        "  #                               #transforms.Lambda(lambda x: torch.cat([x, x, x], dim=0)),\n",
        "  #                               #transforms.Lambda(lambda x: 1.0 if x == 255.0 else x / 255.0),\n",
        "  #                               transforms.ConvertImageDtype(torch.float),\n",
        "  #                               #transforms.Normalize((0.5), (0.5))\n",
        "  #                               ]\n",
        "  # )\n",
        "\n",
        "  data_train = AgroDataset( img_dir_x = TRAIN_IMG_DIR,\n",
        "                            img_dir_y = TRAIN_MASK_DIR,\n",
        "                            transform = train_transform,\n",
        "                            transform_2= test_transform\n",
        "                            #transform = ConvertImageDtype(torch.int),\n",
        "                            #target_transform = ConvertImageDtype(torch.int)\n",
        "                                )\n",
        "  data_test = AgroDataset(  img_dir_x = VAL_IMG_DIR,\n",
        "                            img_dir_y = VAL_MASK_DIR,\n",
        "                            transform = train_transform,\n",
        "                            transform_2= test_transform\n",
        "                            #transform = ConvertImageDtype(torch.int),\n",
        "                            #target_transform = ConvertImageDtype(torch.int)\n",
        "                                )\n",
        "\n",
        "  train_dataloader = DataLoader(\n",
        "                                data_train,\n",
        "                                shuffle=True,\n",
        "                                batch_size=BATCH_SIZE,\n",
        "                                num_workers=NUM_WORKERS,\n",
        "                                drop_last=True\n",
        "                                )\n",
        "\n",
        "  test_dataloader  = DataLoader(\n",
        "                                data_test,\n",
        "                                shuffle=True,\n",
        "                                batch_size=BATCH_SIZE,\n",
        "                                num_workers=NUM_WORKERS\n",
        "                                )\n",
        "\n",
        "  return train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1pmbGuZ26xr"
      },
      "source": [
        "### Testando Num_worker\n",
        "\n",
        "https://chtalhaanwar.medium.com/pytorch-num-workers-a-tip-for-speedy-training-ed127d825db7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1tDAl5g26SV"
      },
      "outputs": [],
      "source": [
        "# from time import time\n",
        "# import multiprocessing as mp\n",
        "# for num_workers in range(2, mp.cpu_count(), 2):\n",
        "#     train_loader , _ = Get_loaders(TRAIN_IMG_DIR = TRAIN_IMG_DIR, \\\n",
        "#                                     TRAIN_MASK_DIR = TRAIN_MASK_DIR, \\\n",
        "#                                     VAL_IMG_DIR = VAL_IMG_DIR, \\\n",
        "#                                     VAL_MASK_DIR = VAL_MASK_DIR, \\\n",
        "#                                     BATCH_SIZE=8, \\\n",
        "#                                     NUM_WORKERS=num_workers, \\\n",
        "#                                     PIN_MEMORY=True)\n",
        "#     start = time()\n",
        "#     for epoch in range(1, 3):\n",
        "#         for (X,y) in train_loader:\n",
        "#             pass\n",
        "#     end = time()\n",
        "#     print(\"Finish with:{} second, num_workers={}\".format(end - start, num_workers))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bIDgv7Y6X7r"
      },
      "source": [
        "### Setup Agnostic code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs4InrC9Qpy_"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sBV0bl5gfm4W",
        "outputId": "ef792d16-1133-448a-910a-a0a572fcd899"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QGvpSu4HL-l"
      },
      "source": [
        "### Importa DeepLabV3+\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M61foWjczWjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e366fd5b-7aeb-4143-d2e2-d3a3d2eb90ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-5d3b4d8f.pth\n",
            "100%|██████████| 170M/170M [00:00<00:00, 190MB/s]\n"
          ]
        }
      ],
      "source": [
        "# import network as net\n",
        "# #é diferente desta maneira que eu fiz ver codigo fonte do git\n",
        "# model = net.deeplabv3plus_resnet101()\n",
        "# net.convert_to_separable_conv(model.classifier)\n",
        "# model = model.to(device)\n",
        "# #utils.set_bn_momentum(model.backbone, momentum=0.01)\n",
        "# #model.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict\n"
      ],
      "metadata": {
        "id": "UJNbX-QQHFhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTkagMJrAuBK"
      },
      "outputs": [],
      "source": [
        "#model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)\n",
        "# or any of these variants\n",
        "#model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', pretrained=True,)\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_mobilenet_v3_large', pretrained=True)\n",
        "#model.eval()\n",
        "#model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zniUM68uJNWv",
        "outputId": "8b953eba-8586-4cc3-843a-7879a934e003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:01<00:00, 102MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models.segmentation import deeplabv3_resnet101 as deeplabv3_resnet101\n",
        "model = deeplabv3_resnet101(num_classes=2)\n",
        "#model.to(\"cpu\")\n",
        "#next(model.parameters()).device\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5AYNGMRIh1G"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6mud1PNtIA0"
      },
      "source": [
        "### Test Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "PVmRWQljhnbG",
        "outputId": "bb49c68c-cefb-4035-d1b7-d2fefe6e8654"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-1c2663b02daa>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Obter um lote de imagens do DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plotar as imagens e mostrar seus shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-18-0c04f9a0718d>\", line 30, in __getitem__\n    image_x = self.train_transform(image_x)\n  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/composition.py\", line 193, in __call__\n    raise KeyError(\"You have to pass data to augmentations as named arguments, for example: aug(image=image)\")\nKeyError: 'You have to pass data to augmentations as named arguments, for example: aug(image=image)'\n"
          ]
        }
      ],
      "source": [
        "# Obter um lote de imagens do DataLoader\n",
        "batch_images, batch_labels = next(iter(test_dataloader))\n",
        "\n",
        "# Plotar as imagens e mostrar seus shapes\n",
        "fig, axs = plt.subplots(2, len(batch_images), figsize=(24, 8))\n",
        "formato = []\n",
        "for i in range(len(batch_images)):\n",
        "    formato.append((batch_images[i].shape,batch_labels[i].shape,'\\n'))\n",
        "    # Converter as imagens e rótulos de tensor para arrays numpy\n",
        "    image_np = batch_images[i].numpy().transpose((1, 2, 0))  # Transpor as dimensões para (height, width, channels)\n",
        "    label_np = batch_labels[i].numpy().transpose((1, 2, 0))  # Transpor as dimensões para (height, width, channels)\n",
        "\n",
        "    # Plotar a imagem original\n",
        "    axs[0, i].imshow(image_np)\n",
        "    axs[0, i].set_title(f'Image Shape: {image_np.shape}')\n",
        "\n",
        "    # Plotar o rótulo\n",
        "    axs[1, i].imshow(label_np)\n",
        "    axs[1, i].set_title(f'Label Shape: {label_np.shape}')\n",
        "\n",
        "# Ajustar espaçamento e exibir o gráfico\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv39C7zbyNWN",
        "outputId": "e24101e1-7ff3-496b-8267-1b1b3221e1b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "valor minimo labels: 0.0\n",
            "valor maximo labels: 0.5176470875740051\n",
            "valor minimo images: 0.0\n",
            "valor maximo images: 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#print(formato)\n",
        "print(f\"\"\"\n",
        "valor minimo labels: {np.min(batch_labels[0:2].numpy())}\n",
        "valor maximo labels: {np.max(batch_labels[0:2].numpy())}\n",
        "valor minimo images: {np.min(batch_images[0:2].numpy())}\n",
        "valor maximo images: {np.max(batch_images[0:2].numpy())}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-dQEndHe6P3"
      },
      "source": [
        "###Setup Loss and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_J5xYQOTig5"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "#torch.optim.lr_scheduler.StepLR(optimizer, step_size= , gamma= )\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amviWiVJavbz"
      },
      "source": [
        "### Utils Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExRS0BzMAPs0"
      },
      "source": [
        "### Metricas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef1T0JmqAPSI"
      },
      "outputs": [],
      "source": [
        "\n",
        "#analise dos resultados\n",
        "def iou_score(preds, targets, smooth=1e-6):\n",
        "    assert preds.shape == targets.shape, \"As dimensões de preds e targets devem ser iguais.\"\n",
        "\n",
        "    preds = preds > 0.5\n",
        "    targets = targets > 0.5\n",
        "\n",
        "    intersection = (preds * targets).sum()\n",
        "    union = preds.sum() + targets.sum() - intersection\n",
        "    iou = (intersection) / (union + smooth)\n",
        "\n",
        "    return iou\n",
        "\n",
        "\n",
        "def dice_score(preds, targets, smooth=1e-6):\n",
        "    intersection = torch.logical_and(preds, targets).sum()\n",
        "    union = preds.sum() + targets.sum()\n",
        "    dice = (2 * intersection + smooth) / (union + smooth)\n",
        "    return dice\n",
        "\n",
        "def recall(preds, targets, smooth=1e-6):\n",
        "   #binariza as previsões e os alvos\n",
        "    preds = preds > 0.5\n",
        "    targets = targets > 0.5\n",
        "\n",
        "    # calcula o número de verdadeiros positivos e falsos negativos\n",
        "    true_positives = torch.logical_and(preds, targets).sum()\n",
        "    false_negatives = targets.sum() - true_positives\n",
        "\n",
        "     # calcula o recall com suavização para evitar divisão por zero\n",
        "    recall_value = true_positives / (true_positives + false_negatives + smooth)\n",
        "    return recall_value\n",
        "\n",
        "def falso_positivo(preds, targets, smooth=1e-6):\n",
        "    #binarizo as previsões e alvos\n",
        "    preds = preds > 0.5 #qualquer valor acima de 0.5 é positivo e abaixo de 0.5 é negativo\n",
        "    targets = targets > 0.5\n",
        "\n",
        "    falsos_positivos = (preds > targets).sum()  #calcula o número de falsos positivos\n",
        "\n",
        "    verdadeiros_negativos = (~preds & ~targets).sum()  #uso ~como negativo, ou seja quando ambos são 0, tanto a máscara como a predição.\n",
        "\n",
        "    fpr = falsos_positivos.sum() / (falsos_positivos.sum() + verdadeiros_negativos.sum() + smooth)    #fórmula + smooth para evitar divisão por 0\n",
        "    return fpr\n",
        "\n",
        "def falso_negativo(preds, targets, smooth=1e-6):\n",
        "    #binarizo as previsões e alvos\n",
        "    preds = preds > 0.5 #qualquer valor acima de 0.5 é positivo e abaixo de 0.5 é negativo\n",
        "    targets = targets > 0.5\n",
        "\n",
        "    verdadeiros_positivos = torch.logical_and(targets, preds).sum() #usa essa função lógica do pytorch para calcular os acertos\n",
        "\n",
        "    falsos_negativos = targets.sum() - verdadeiros_positivos # aqui faço:  o total de positivos certo - o total de positivos acertados = o número de positivos que foi predito como negativos\n",
        "\n",
        "    fnr = falsos_negativos.sum() / (falsos_negativos.sum() + verdadeiros_positivos.sum() + smooth) #fórmula + smooth para evitar divisão por 0\n",
        "\n",
        "    return fnr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tgm5gZ5AZeN"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "\n",
        "def criar_ou_verificar_pasta(folder_name):\n",
        "    \"\"\"\n",
        "    Verifica se uma pasta já existe e, se não existir, cria a pasta.\n",
        "\n",
        "    Args:\n",
        "        folder_name (str): Nome da pasta que você deseja criar ou verificar.\n",
        "\n",
        "    Returns:\n",
        "        bool: True se a pasta foi criada, False se a pasta já existia.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(folder_name):\n",
        "        os.mkdir(folder_name)\n",
        "        print(f'A pasta \"{folder_name}\" foi criada.')\n",
        "        return True\n",
        "    else:\n",
        "        print(f'A pasta \"{folder_name}\" já existe e não foi criada novamente.')\n",
        "        return False\n",
        "def save_predictions_as_imgs(\n",
        "    loader, model, folder=\"/content/gdrive/MyDrive/DeepLabV3Plus-Pytorch/checkpoints\", device=\"cuda\"\n",
        "):\n",
        "    model.eval()\n",
        "    for idx, (x, y) in enumerate (loader):\n",
        "        x = x.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds > 0.5).float()\n",
        "\n",
        "\n",
        "        save_image(\n",
        "            preds, f\"{folder}/pred_{idx}.png\"\n",
        "        )\n",
        "        save_image(y.unsqueeze(1), f\"{folder}/true_{idx}.png\")\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoWAWHdu-eyd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "14e90162-3e80-4e24-97db-0328b6e7e77c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-9fa0cc036272>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#assert preds.shape == x.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-9fa0cc036272>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/segmentation/_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# contract: features is a dict of tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mout_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
          ]
        }
      ],
      "source": [
        "def test_model():\n",
        "    x = torch.rand((3, 3, 255, 255))\n",
        "    preds = model(x)['out']\n",
        "    print(preds.shape)\n",
        "    print(x.shape)\n",
        "    #assert preds.shape == x.shape\n",
        "test_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg3_yUHtC9iX"
      },
      "outputs": [],
      "source": [
        "def train_step(model,\n",
        "               data_loader,\n",
        "               loss_fn,\n",
        "               optimizer,\n",
        "               device,\n",
        "               scaler):\n",
        "\n",
        "  train_loss = 0.0\n",
        "  model.train()\n",
        "\n",
        "  for batch, (X, y) in enumerate(data_loader):\n",
        "    X, y = X.to(device), y.squeeze(1).type(torch.long).to(device)\n",
        "\n",
        "    with torch.cuda.amp.autocast():\n",
        "      y_pred = model(X)['out']\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      #wandb.log({\"train_loss\": loss})\n",
        "\n",
        "    train_loss += loss\n",
        "    optimizer.zero_grad()\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # Calculate loss and accuracy per epoch and print out what's happening\n",
        "    train_loss /= len(data_loader)\n",
        "    # train_acc /= len(data_loader.Dataset)\n",
        "    if batch % 2 == 0:\n",
        "      print(f\"\"\"Batch: {batch}\n",
        "      Train loss: {train_loss/len(data_loader):.5f}\n",
        "            \"\"\")\n",
        "      #print(\"checkpoint\")\n",
        "\n",
        "\n",
        "def test_step(data_loader,\n",
        "              model,\n",
        "              loss_fn,\n",
        "              device):\n",
        "  \"\"\"\n",
        "  Perform a testing loop step on model going over data_loader.\n",
        "  \"\"\"\n",
        "  num_correct = 0\n",
        "  num_pixels = 0\n",
        "  dice_score_total = 0\n",
        "  iou = 0.0\n",
        "  recall_total = 0.0\n",
        "  fpr_total = 0.0\n",
        "  fnr_total = 0.0\n",
        "\n",
        "\n",
        "  model.eval()\n",
        "  # Turn on inference context manager\n",
        "  with torch.no_grad():\n",
        "    for X, y in data_loader:\n",
        "\n",
        "      X, y = X.to(device), y.squeeze(1).type(torch.long).to(device)\n",
        "      test_pred = model(X)['out']\n",
        "      preds = torch.sigmoid(test_pred)\n",
        "      preds = torch.sum(preds, dim=1)\n",
        "      preds = (preds > 0.5).float() # Aplicação de threshold\n",
        "\n",
        "      # contagem de pixels corretos\n",
        "      num_correct = torch.sum((preds == y).float()).item()\n",
        "      num_pixels += torch.numel(preds)\n",
        "\n",
        "      # cálculo das métricas\n",
        "      dice_score_total += dice_score(preds, y)\n",
        "      iou += iou_score(preds, y)\n",
        "      recall_total += recall(preds, y)\n",
        "      fpr_total += falso_positivo(preds, y)\n",
        "      fnr_total += falso_negativo(preds, y)\n",
        "\n",
        "  # \"\"\"Métricas no wandb\"\"\"\n",
        "  # wandb.log({\"Acuracy\": num_correct / num_pixels, \"Dice_score\": dice_score_total / len(data_loader), \"IOU_score\": iou / len(data_loader), \"Recall\": recall_total / len(data_loader)})\n",
        "  # wandb.log({\"Falso positivo\": fpr_total / len(data_loader), \"Falso Negativo\": fnr_total / len(data_loader)})\n",
        "\n",
        "  print(f\"\"\"Validação\n",
        "  tamanho data loader: {len(data_loader)}\n",
        "  | Pixeis corretos |: {num_correct:.4f}/\n",
        "  | Acc |: {num_correct / num_pixels * 100:.4f}\n",
        "  | Dice Score|: {dice_score_total / len(data_loader):.4f}\n",
        "  | IoU Score|: {iou / len(data_loader):.4f}\n",
        "  | Recall|: {recall_total / len(data_loader):.4f}\n",
        "  | Taxa de falsos positivos|: {fpr_total/ len(data_loader):.4f}\n",
        "  | Taxa de falsos Negativos|: {fnr_total/ len(data_loader):.4f}\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "def eval_model(model,\n",
        "               data_loader,\n",
        "               loss_fn,\n",
        "               accuracy_fn):\n",
        "  \"\"\"\n",
        "  Returns a dictionary containing the results of model predictiong on data_loader.\n",
        "  \"\"\"\n",
        "\n",
        "  loss, acc = 0, 0\n",
        "  with torch.inference_mode():\n",
        "    for X, y in tqdm(data_loader):\n",
        "      X, y = X.to(device), y.squeeze(1).type(torch.long).to(device)\n",
        "      y_pred = model(X)['out']\n",
        "      loss += loss_fn(y_pred, y)\n",
        "\n",
        "    #Scale loss and acc to find the average loss/acc per batch\n",
        "    loss /= len(data_loader)\n",
        "    acc /= len(data_loader)\n",
        "  return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "          \"model_loss\": loss.item(),\n",
        "          \"model_acc\": acc\n",
        "          }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n"
      ],
      "metadata": {
        "id": "fdfHQLPS-ZfH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7Z_TtwVF7FI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "743ccdd2-9ebe-425e-f205-ecf3c6f2cbaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "train_dataloader, test_dataloader = Get_loaders(TRAIN_IMG_DIR = TRAIN_IMG_DIR, \\\n",
        "                                                TRAIN_MASK_DIR = TRAIN_MASK_DIR, \\\n",
        "                                                VAL_IMG_DIR = VAL_IMG_DIR, \\\n",
        "                                                VAL_MASK_DIR = VAL_MASK_DIR, \\\n",
        "                                                BATCH_SIZE=8, \\\n",
        "                                                NUM_WORKERS=4, \\\n",
        "                                                PIN_MEMORY=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlOMH-jrEOZR",
        "outputId": "6f8cfd32-c9ec-49ca-f122-3f2a8960060c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A pasta \"checkpoints\" já existe e não foi criada novamente.\n",
            "checkpoints  Heart.csv\t\t  main.py  predict.py\trequirements.txt  wandb\n",
            "dataset      helper_functions.py  metrics  __pycache__\tsamples\n",
            "datasets     LICENSE\t\t  network  README.md\tutils\n"
          ]
        }
      ],
      "source": [
        "criar_ou_verificar_pasta(\"checkpoints\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main\n"
      ],
      "metadata": {
        "id": "ga8Lo8ZO-cdF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lGMAVQzEdRm"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "#Meansure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_on_gpu = timer()\n",
        "\n",
        "# Parametros\n",
        "epochs = 2\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Create a optimization and evaluation loop using train_step() and test_step()\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n------------\\n\")\n",
        "  train_step(model=model,\n",
        "            data_loader=train_dataloader,\n",
        "            loss_fn=criterion,\n",
        "            optimizer=optimizer,\n",
        "            device=device,\n",
        "            scaler=scaler)\n",
        "  test_step(model=model,\n",
        "            data_loader = test_dataloader,\n",
        "            loss_fn=criterion,\n",
        "            device=device)\n",
        "\n",
        "  train_time_end_on_gpu = timer()\n",
        "  total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
        "                                              end=train_time_end_on_gpu,\n",
        "                                              device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ab9b71e0230d4f438fbee0d62887edb5",
            "a5f337e80cef4feb8109686fe62341f6",
            "caf550fb623f4eefa3ad60301b1be746",
            "5a545309252c4a03906a8b15a38c743f",
            "20321a7f380f4ae5a11dac78e9d629f6",
            "6621df4486ea4cc49e729c482c247075",
            "57a04823d011420ab8a4886751917bd6",
            "4e0744cf28f54fdfbd11a310583faa28",
            "352fb83e2ed94790bfb5c85ad12d137c",
            "828ade71faaf4fda9c890462b7e2aaa8",
            "b481def6e352489aaa6789680e3d63a6"
          ]
        },
        "id": "bPA8NCfPHi_h",
        "outputId": "48a18e6a-90e1-44ed-aec0-cb122c276396"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab9b71e0230d4f438fbee0d62887edb5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "# Get model_1 results dictionary\n",
        "model_1_results = eval_model(model=model,\n",
        "                             data_loader=test_dataloader,\n",
        "                             loss_fn=criterion,\n",
        "                             accuracy_fn=accuracy_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtwALwYuLRMx"
      },
      "source": [
        "Usar 🇰* https://pytorch.org/vision/stable/models/generated/torchvision.models.segmentation.deeplabv3_resnet101.html#torchvision.models.segmentation.deeplabv3_resnet101"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "q6ahOXqrsO20",
        "Feeg3yHGGe5h",
        "DgyuakE5YJcp",
        "YoxLNZEBdACX",
        "r6mud1PNtIA0",
        "v1pmbGuZ26xr"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20321a7f380f4ae5a11dac78e9d629f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "352fb83e2ed94790bfb5c85ad12d137c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e0744cf28f54fdfbd11a310583faa28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57a04823d011420ab8a4886751917bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a545309252c4a03906a8b15a38c743f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_828ade71faaf4fda9c890462b7e2aaa8",
            "placeholder": "​",
            "style": "IPY_MODEL_b481def6e352489aaa6789680e3d63a6",
            "value": " 4/4 [00:03&lt;00:00,  1.91it/s]"
          }
        },
        "6621df4486ea4cc49e729c482c247075": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "828ade71faaf4fda9c890462b7e2aaa8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5f337e80cef4feb8109686fe62341f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6621df4486ea4cc49e729c482c247075",
            "placeholder": "​",
            "style": "IPY_MODEL_57a04823d011420ab8a4886751917bd6",
            "value": "100%"
          }
        },
        "ab9b71e0230d4f438fbee0d62887edb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5f337e80cef4feb8109686fe62341f6",
              "IPY_MODEL_caf550fb623f4eefa3ad60301b1be746",
              "IPY_MODEL_5a545309252c4a03906a8b15a38c743f"
            ],
            "layout": "IPY_MODEL_20321a7f380f4ae5a11dac78e9d629f6"
          }
        },
        "b481def6e352489aaa6789680e3d63a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "caf550fb623f4eefa3ad60301b1be746": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e0744cf28f54fdfbd11a310583faa28",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_352fb83e2ed94790bfb5c85ad12d137c",
            "value": 4
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}