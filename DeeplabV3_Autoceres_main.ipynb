{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gusanagy/DeepLabV3/blob/main/DeeplabV3_Autoceres_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfXEOh7fE3Mb"
      },
      "source": [
        "#DeeplabV3+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfKfbN5Wr56z"
      },
      "source": [
        "* investigar collate function\n",
        "\n",
        "* Atualizar no github\n",
        "* Organizar codigo\n",
        "\n",
        "* ajustar passo de treino\n",
        "* debugar erros no formato batches incompatible\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk5t2GCtGZ1o"
      },
      "source": [
        "##Bibliotecas usadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy_7V0RnvhjX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from albumentations import Compose, HorizontalFlip, ChannelShuffle, Resize, CoarseDropout, Rotate, Equalize, RandomShadow, CenterCrop, RandomBrightnessContrast, HorizontalFlip, CLAHE, Normalize\n",
        "#from albumentations.augmentations.geometric.resize import Resize as\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch import nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Resize\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms import ConvertImageDtype\n",
        "from torchvision import datasets, transforms\n",
        "#from torchvision.transforms import Compose\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qus1XC8FHJJF"
      },
      "source": [
        "## Montar Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z2IwuLgsNVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f182912-9311-4efe-a303-522c20ecbb95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n",
            "dataset   .gitignore\t       LICENSE\tnetwork      README.md\t       utils\n",
            "datasets  Heart.csv\t       main.py\tpredict.py   requirements.txt\n",
            ".git\t  helper_functions.py  metrics\t__pycache__  samples\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# montar Google Drive / mount Google drive\n",
        "# será solicitadas as credenciais do usuario\n",
        "# do gmail\n",
        "drive.mount('/content/gdrive/',force_remount=True)\n",
        "#ir para pasta conhecimento\n",
        "path = '/content/gdrive/MyDrive/DeepLabV3Plus-Pytorch/'\n",
        "#ir para pasta conhecimento\n",
        "os.chdir(path)\n",
        "\n",
        "#listar arquivos da pasta\n",
        "!ls -A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bIDgv7Y6X7r"
      },
      "source": [
        "### Setup Agnostic code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs4InrC9Qpy_"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBV0bl5gfm4W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2dd0dd2e-77b7-40ec-995d-007dde9df2f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Feeg3yHGGe5h"
      },
      "source": [
        "##Data Augmentation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY0SPQFmR5A6"
      },
      "outputs": [],
      "source": [
        "def augment_data(images, masks, save_path, augment = True):\n",
        "    H = 512\n",
        "    W = 512\n",
        "\n",
        "    for x, y in tqdm(zip(images, masks), total = len(images)):\n",
        "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "        x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "        y = cv2.imread(y, cv2.IMREAD_COLOR)\n",
        "\n",
        "        \"\"\" Data Augmentation \"\"\"\n",
        "        if augment == True:\n",
        "            aug = HorizontalFlip(p = 1.0) #Flip horizontal\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x1 = augmented[\"image\"]\n",
        "            y1 = augmented[\"mask\"]\n",
        "\n",
        "            x2 = cv2.cvtColor(x, cv2.COLOR_RGB2GRAY)\n",
        "            y2 = y\n",
        "\n",
        "            \"\"\"\n",
        "            aug = ChannelShuffle(p = 1) #Reorganiza aleatoriamente os canais da imagem RGB de entrada.\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x3 = augmented['image']\n",
        "            y3 = augmented['mask']\n",
        "            \"\"\"\n",
        "\n",
        "            aug = CoarseDropout(p = 1, min_holes = 3, max_holes = 10, max_height = 32, max_width = 32) #Dropa uma caixa de pixels 32x32 da imagem, pode simular possíveis falhas na linha de plantação\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x4 = augmented['image']\n",
        "            y4 = augmented['mask']\n",
        "\n",
        "            \"\"\"\n",
        "            aug = Rotate(limit = 45, p = 1.0) #Rotaciona a imagem de entrada\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x5 = augmented[\"image\"]\n",
        "            y5 = augmented[\"mask\"]\n",
        "            \"\"\"\n",
        "\n",
        "            aug = Equalize(p = 1.0) #Equaliza a imagem de entrada\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x5 = augmented[\"image\"]\n",
        "            y5 = augmented[\"mask\"]\n",
        "\n",
        "            \"\"\"\n",
        "            aug = RandomShadow(shadow_roi = (0, 0.5, 1, 1), num_shadows_lower = 1, num_shadows_upper = 1, shadow_dimension = 4, always_apply = False, p=1) #Cria sombras nas imagens\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x6 = augmented[\"image\"]\n",
        "            y6 = augmented[\"mask\"]\n",
        "            \"\"\"\n",
        "\n",
        "            aug = RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1.5) #Brilho aleatório\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x6 = augmented[\"image\"]\n",
        "            y6 = augmented[\"mask\"]\n",
        "\n",
        "            X = [x, x1, x2, x4, x5, x6]\n",
        "            Y = [y, y1, y2, y4, y5, y6]\n",
        "\n",
        "        else:\n",
        "            X = [x]\n",
        "            Y = [y]\n",
        "\n",
        "        index = 0\n",
        "        for i, m in zip(X, Y):\n",
        "            try:\n",
        "                aug = CenterCrop(H, W, p = 1.0)\n",
        "                augmented = aug(image = i, mask = m)\n",
        "                i = augmented[\"image\"]\n",
        "                m = augmented[\"mask\"]\n",
        "\n",
        "            except Exception as e:\n",
        "                i = cv2.resize(i, (W, H))\n",
        "                m = cv2.resize(m, (W, H))\n",
        "\n",
        "            tmp_image_name = f\"{name}_{index}.png\"\n",
        "            tmp_mask_name = f\"{name}_{index}.png\"\n",
        "\n",
        "            image_path = os.path.join(save_path, \"image\", tmp_image_name)\n",
        "            mask_path = os.path.join(save_path, \"mask\", tmp_mask_name)\n",
        "\n",
        "            cv2.imwrite(image_path, i)\n",
        "            cv2.imwrite(mask_path, m)\n",
        "\n",
        "            index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIlumo8RRwe-"
      },
      "outputs": [],
      "source": [
        "\"\"\" Diretório \"\"\"\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def load_data(path, split = 0.25):\n",
        "    \"\"\" Carregamento das imagens e máscaras \"\"\"\n",
        "    X = sorted(glob(os.path.join(path, \"/content/gdrive/MyDrive/DeepLabV3Plus-Pytorch/datasets/Dataset_Strawberry/images\", \"*.png\")))\n",
        "    Y = sorted(glob(os.path.join(path, \"/content/gdrive/MyDrive/DeepLabV3Plus-Pytorch/datasets/Dataset_Strawberry/masks\", \"*.png\")))\n",
        "\n",
        "    split_size = int(len(X) * split)\n",
        "\n",
        "    train_x, test_x = train_test_split(X, test_size = split_size, random_state = 42)\n",
        "    train_y, test_y = train_test_split(Y, test_size = split_size, random_state = 42)\n",
        "\n",
        "    return (train_x, train_y), (test_x, test_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgyuakE5YJcp"
      },
      "source": [
        "## Data Augmentation Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkdQnNGE5UUH"
      },
      "outputs": [],
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     np.random.seed(42)\n",
        "\n",
        "#     \"\"\" Carregamento do Dataset \"\"\"\n",
        "#     data_path = \"datasets\"\n",
        "\n",
        "#     (train_x, train_y), (test_x, test_y) = load_data(data_path)\n",
        "\n",
        "#     print(f\"Train:\\t {len(train_x)} - {len(train_y)}\")\n",
        "#     print(f\"Test:\\t {len(test_x)} - {len(test_y)}\")\n",
        "\n",
        "#     \"\"\" Criando os diretórios para o Data Augmentation \"\"\"\n",
        "#     create_dir(\"/dataset/new_data/train/image/\")\n",
        "#     create_dir(\"/dataset/new_data/train/mask/\")\n",
        "#     create_dir(\"/dataset/new_data/test/image/\")\n",
        "#     create_dir(\"/dataset/new_data/test/mask/\")\n",
        "\n",
        "#     augment_data(train_x, train_y, \"/dataset/new_data/train/\", augment = True)\n",
        "#     augment_data(test_x, test_y, \"/dataset/new_data/test/\", augment = True)#Substitui por True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhfLNhJlYOkw"
      },
      "source": [
        "## Setup DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoxLNZEBdACX"
      },
      "source": [
        "### Endereço para o dataset Augumentado com a lib augumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f9OCzjNjX7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "361fedf4-e4e3-4ee3-ce5c-db59e6702203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "          Dir of X / no aug:     datasets/Dataset_Strawberry/images/\n",
            "          Dir of y / no aug:     datasets/Dataset_Strawberry/masks/\n",
            "          Dir train x:  dataset/new_data/train/image/\n",
            "          Dir train y:  dataset/new_data/train/mask/\n",
            "          Dir test x:   dataset/new_data/test/image/\n",
            "          Dir test y:   dataset/new_data/test/mask/\n"
          ]
        }
      ],
      "source": [
        "#diretório das imagens de treino\n",
        "TRAIN_IMG_DIR = \"dataset/new_data/train/image/\"\n",
        " #diretório das máscaras de treino\n",
        "TRAIN_MASK_DIR = \"dataset/new_data/train/mask/\"\n",
        "#diretório das imagens de teste\n",
        "VAL_IMG_DIR = \"dataset/new_data/test/image/\"\n",
        "#diretório das máscaras de teste\n",
        "VAL_MASK_DIR = \"dataset/new_data/test/mask/\"\n",
        "\n",
        "#Caso fosse utilizar o dataset sem augumentar\n",
        "DIR_X = \"datasets/Dataset_Strawberry/images/\"\n",
        "DIR_y = \"datasets/Dataset_Strawberry/masks/\"\n",
        "\n",
        "print(f\"\"\"\n",
        "          Dir of X / no aug:     {DIR_X}\n",
        "          Dir of y / no aug:     {DIR_y}\n",
        "          Dir train x:  {TRAIN_IMG_DIR}\n",
        "          Dir train y:  {TRAIN_MASK_DIR}\n",
        "          Dir test x:   {VAL_IMG_DIR}\n",
        "          Dir test y:   {VAL_MASK_DIR}\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZuUivU9dPOo"
      },
      "source": [
        "### Classe Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQzvtBuuzx5v"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def one_hot_encode(label, label_values):\n",
        "    \"\"\"\n",
        "    Convert a segmentation image label array to one-hot format\n",
        "    by replacing each pixel value with a vector of length num_classes\n",
        "    # Arguments\n",
        "        label: The 2D array segmentation image label\n",
        "        label_values\n",
        "\n",
        "    # Returns\n",
        "        A 2D array with the same width and hieght as the input, but\n",
        "        with a depth size of num_classes\n",
        "    \"\"\"\n",
        "    semantic_map = []\n",
        "    for colour in label_values:\n",
        "        equality = np.equal(label, colour)\n",
        "        class_map = np.all(equality, axis = -1)\n",
        "        semantic_map.append(class_map)\n",
        "    semantic_map = np.stack(semantic_map, axis=-1)\n",
        "\n",
        "    return semantic_map\n",
        "\n",
        "class AgroDataset(Dataset):\n",
        "    def __init__(self, img_dir_x,img_dir_y, transform=None,transform_2=None):\n",
        "        #self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir_x = img_dir_x\n",
        "        self.img_dir_y = img_dir_y\n",
        "        self.train_transform = transform\n",
        "        self.train_transform_2 = transform_2\n",
        "        self.images = os.listdir(img_dir_x)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_dir_x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path_x = os.path.join(self.img_dir_x,self.images[idx].replace(\".jpg\", \".png\"))\n",
        "        img_path_y = os.path.join(self.img_dir_y,self.images[idx].replace(\".jpg\", \".png\"))\n",
        "        image_x =  np.array(Image.open(img_path_x).convert(\"RGB\"))#Modifiqui para nao carretgar imagens rgb\n",
        "        label_mask_y = np.array(Image.open(img_path_y).convert(\"L\"))\n",
        "\n",
        "        #label_mask_y = label_mask_y.repeat(3, 1, 1)\n",
        "        #image_x = read_image(img_path_x)#.numpy().dtype(np.int32)\n",
        "        #label_mask_y = read_image(img_path_y)#.numpy().dtype(np.int32)\n",
        "        #label_mask_y = Image.open(img_path_y)\n",
        "        #image_x = Image.open(img_path_x)\n",
        "        #label = os.path.basename(X_train[idx])\n",
        "        #label_mask_y[label_mask_y ==255.0] = 1.0\n",
        "\n",
        "        #convert_to_binary = lambda x: 1.0 if x == 255 else 0.0\n",
        "        #resultado = list(map(convert_to_binary, label_mask_y))\n",
        "\n",
        "        if self.train_transform is not None:\n",
        "          image_x = self.train_transform(image_x)\n",
        "          label_mask_y = self.train_transform_2(label_mask_y)\n",
        "        return image_x, label_mask_y\n",
        "    # classmethod\n",
        "    # def decode_target(cls, mask):\n",
        "    #     \"\"\"decode semantic mask to RGB image\"\"\"\n",
        "    #     return cls.cmap[mask]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvFlwdZedUIJ"
      },
      "source": [
        "###Instancia a Classe e carrega os dados do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcqvS_rzacmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3896c0f-cdbe-4083-b00c-6b285e9750b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "IMAGE_HEIGHT = 160\n",
        "IMAGE_WIDTH = 240\n",
        "BATCH = 8\n",
        "WORKERS = 4\n",
        "\n",
        "\n",
        "# Definir transformações para o DataLoader\n",
        "train_transform = transforms.Compose(\n",
        "                              [\n",
        "                              transforms.ToPILImage(),\n",
        "                              transforms.Resize((255,255)),\n",
        "                              transforms.ToTensor(),\n",
        "                              #transforms.Lambda(lambda x: torch.cat([x, x, x], dim=0)),\n",
        "                              #transforms.Lambda(lambda x: 1.0 if x == 255.0 else x / 255.0),\n",
        "                              #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "                              #transforms.Normalize((0.485), (0.229)),\n",
        "                              transforms.ConvertImageDtype(torch.float)\n",
        "                              ]\n",
        ")\n",
        "train_transform_2 = transforms.Compose(\n",
        "                              [\n",
        "                              transforms.ToPILImage(),\n",
        "                              transforms.Resize((255,255)),\n",
        "                              transforms.ToTensor(),\n",
        "                              #transforms.Lambda(lambda x: torch.cat([x, x, x], dim=0)),\n",
        "                              #transforms.Lambda(lambda x: 1.0 if x == 255.0 else x / 255.0),\n",
        "                              #transforms.Normalize((0.485), (0.229)),\n",
        "                              transforms.ConvertImageDtype(torch.float)\n",
        "                              ]\n",
        ")\n",
        "\n",
        "\n",
        "# train_transform = Compose(\n",
        "#     [\n",
        "#         Resize((IMAGE_HEIGHT,IMAGE_WIDTH)),\n",
        "#         Rotate(limit=35, p=0.3),                        #Rotaciona em até 35 graus\n",
        "#         HorizontalFlip(p=1.0),                          #Espelha horizontalmente com probabilidade de 0,5\n",
        "#         CLAHE(p=0.2),                                   #ajuda a melhorar a visibilidade de detalhes em regiões escuras ou claras da imagem.\n",
        "#         Normalize(                   #normaliza os valores de pixel da imagem para ter média 0 e desvio padrão 1\n",
        "#             mean=[0.5, 0.5, 0.5],\n",
        "#             std=[0.5, 0.5, 0.5],\n",
        "#             max_pixel_value=255.0,\n",
        "#         ),\n",
        "#         ToTensorV2(),     #converte a imagem para o formato tensor\n",
        "#     ],\n",
        "# )\n",
        "# val_transforms = Compose(\n",
        "#         [\n",
        "#             Resize((IMAGE_HEIGHT,IMAGE_WIDTH)),   #redimensiona a imagem\n",
        "#             Normalize(\n",
        "#                 mean=[0.5, 0.5, 0.5],\n",
        "#                 std=[0.5, 0.5, 0.5],\n",
        "#                 max_pixel_value=255.0,\n",
        "#             ),\n",
        "#             ToTensorV2(),\n",
        "#         ],\n",
        "#     )\n",
        "\n",
        "data_train = AgroDataset( img_dir_x = TRAIN_IMG_DIR,\n",
        "                          img_dir_y = TRAIN_MASK_DIR,\n",
        "                          transform = train_transform,\n",
        "                          transform_2=train_transform_2\n",
        "                          #transform = ConvertImageDtype(torch.int),\n",
        "                          #target_transform = ConvertImageDtype(torch.int)\n",
        "                              )\n",
        "data_test = AgroDataset(  img_dir_x = VAL_IMG_DIR,\n",
        "                          img_dir_y = VAL_IMG_DIR,\n",
        "                          transform = train_transform,\n",
        "                          transform_2=train_transform_2\n",
        "                          #transform = ConvertImageDtype(torch.int),\n",
        "                          #target_transform = ConvertImageDtype(torch.int)\n",
        "                              )\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "                              data_train,\n",
        "                              shuffle=True,\n",
        "                              batch_size=BATCH,\n",
        "                              num_workers=WORKERS,\n",
        "                              drop_last=True\n",
        "                              )\n",
        "\n",
        "test_dataloader  = DataLoader(\n",
        "                              data_test,\n",
        "                              shuffle=True,\n",
        "                              batch_size=BATCH,\n",
        "                              num_workers=WORKERS\n",
        "                              )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g92m9cs0CA-f"
      },
      "source": [
        "Fazer um preprocessing que seriva para os dois??\n",
        "\n",
        "Carregar os dados de fomra diferente?\n",
        "\n",
        "Implementar on-hot encode?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7Z_TtwVF7FI"
      },
      "outputs": [],
      "source": [
        "#batch_images, batch_labels = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVmRWQljhnbG"
      },
      "outputs": [],
      "source": [
        "# # Obter um lote de imagens do DataLoader\n",
        "# batch_images, batch_labels = next(iter(train_dataloader))\n",
        "\n",
        "# # Plotar as imagens e mostrar seus shapes\n",
        "# fig, axs = plt.subplots(2, len(batch_images), figsize=(24, 8))\n",
        "# formato = []\n",
        "# for i in range(len(batch_images)):\n",
        "#     formato.append((batch_images[i].shape,batch_labels[i].shape,'\\n'))\n",
        "#     # Converter as imagens e rótulos de tensor para arrays numpy\n",
        "#     image_np = batch_images[i].numpy().transpose((1, 2, 0))  # Transpor as dimensões para (height, width, channels)\n",
        "#     label_np = batch_labels[i].numpy().transpose((1, 2, 0))  # Transpor as dimensões para (height, width, channels)\n",
        "\n",
        "#     # Plotar a imagem original\n",
        "#     axs[0, i].imshow(image_np)\n",
        "#     axs[0, i].set_title(f'Image Shape: {image_np.shape}')\n",
        "\n",
        "#     # Plotar o rótulo\n",
        "#     axs[1, i].imshow(label_np)\n",
        "#     axs[1, i].set_title(f'Label Shape: {label_np.shape}')\n",
        "\n",
        "# # Ajustar espaçamento e exibir o gráfico\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv39C7zbyNWN"
      },
      "outputs": [],
      "source": [
        "# #print(formato)\n",
        "# print(f\"\"\"\n",
        "# {batch_labels[0]}\n",
        "\n",
        "# {batch_images[0]}\n",
        "\n",
        "# \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1pmbGuZ26xr"
      },
      "source": [
        "### Testando Num_worker\n",
        "\n",
        "https://chtalhaanwar.medium.com/pytorch-num-workers-a-tip-for-speedy-training-ed127d825db7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1tDAl5g26SV"
      },
      "outputs": [],
      "source": [
        "# from time import time\n",
        "# import multiprocessing as mp\n",
        "# for num_workers in range(2, mp.cpu_count(), 2):\n",
        "#     train_loader = DataLoader(data_test,shuffle=True,num_workers=num_workers,batch_size=8,pin_memory=True)\n",
        "#     start = time()\n",
        "#     for epoch in range(1, 3):\n",
        "#         for i, data in enumerate(train_loader, 0):\n",
        "#             pass\n",
        "#     end = time()\n",
        "#     print(\"Finish with:{} second, num_workers={}\".format(end - start, num_workers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP7UwA64ctK2"
      },
      "source": [
        "###exemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOx4xYwCgiED"
      },
      "outputs": [],
      "source": [
        "# #Exemplo seguido\n",
        "# data = CustomImageDataset(X, y)\n",
        "\n",
        "# train_size = int(0.75*len(data))\n",
        "# idx  = torch.randperm(len(data))\n",
        "# train_sampler = SubsetRandomSampler(idx[0:train_size])\n",
        "# test_sampler = SubsetRandomSampler(idx[train_size:])\n",
        "\n",
        "# train_loader = DataLoader(data, sampler=train_sampler,\n",
        "#                           batch_size=10, num_workers=2)\n",
        "\n",
        "# test_loader  = DataLoader(data, sampler=test_sampler,\n",
        "#                           batch_size=10, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QGvpSu4HL-l"
      },
      "source": [
        "## Importa DeepLabV3+\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M61foWjczWjO"
      },
      "outputs": [],
      "source": [
        "# import network as net\n",
        "# #é diferente desta maneira que eu fiz ver codigo fonte do git\n",
        "# model = net.deeplabv3plus_resnet101()\n",
        "# net.convert_to_separable_conv(model.classifier)\n",
        "# model = model.to(device)\n",
        "# #utils.set_bn_momentum(model.backbone, momentum=0.01)\n",
        "# #model.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTkagMJrAuBK"
      },
      "outputs": [],
      "source": [
        "#model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)\n",
        "# or any of these variants\n",
        "#model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet101', pretrained=True,)\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_mobilenet_v3_large', pretrained=True)\n",
        "#model.eval()\n",
        "#model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zniUM68uJNWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44080530-5922-4c77-c104-2a48ea3f1777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:00<00:00, 287MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models.segmentation import deeplabv3_resnet101 as deeplabv3_resnet101\n",
        "model = deeplabv3_resnet101(num_classes=2)\n",
        "#model.to(\"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-dQEndHe6P3"
      },
      "source": [
        "###Setup Loss and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_J5xYQOTig5"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "#torch.optim.lr_scheduler.StepLR(optimizer, step_size= , gamma= )\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysiyDSZx37vd"
      },
      "source": [
        "##Treino\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6ahOXqrsO20"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR7odsiNoZ-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4784efdb-de64-4fce-d3ec-ab02a81e083f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "helper_functions.py already exists, skipping download\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  # Note: you need the \"raw\" GitHub URL for this to work\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    f.write(request.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Fr_zuauo4tM"
      },
      "outputs": [],
      "source": [
        "# Import accuracy metric\n",
        "from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "def print_train_time(start: float,\n",
        "                     end: float,\n",
        "                     device: torch.device = None):\n",
        "  \"\"\"Prints difference between start and end time. \"\"\"\n",
        "  total_time = end-start\n",
        "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "  return total_time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict"
      ],
      "metadata": {
        "id": "Z8GFAvGGc942"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpbBxoD4sSUc"
      },
      "source": [
        "### Treino"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH2ADC-sGUpv"
      },
      "source": [
        "* 21 classes de saida. Talvez terei que doiminuir o numero de classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24srjXUVA62F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exux1gW8orq-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538,
          "referenced_widgets": [
            "6f419e594619498c91ab73993f2698bb",
            "3e0317fe613448bd9b1c5c9c5022bfad",
            "4f0ed539cfea488db5a2c66da32452b2",
            "94ef2584fe0f4ff7b69736e416de795b",
            "dfd2b4d672b74214b5baf7eb77fa1c5a",
            "1046266cc38b402183ac86b231f57f57",
            "8ee02b09a4b446fda631862be1c587db",
            "384dce75651f4f838f5d08a20e4b5141",
            "f2c9a328adeb4e979b188d07cba60f44",
            "679528e9b8a04961993157644bbfbb0e",
            "9230ac41958e4fb9a462c05b776426bd"
          ]
        },
        "outputId": "bf9885d8-79f3-4b6d-b398-a5baf6733d3a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f419e594619498c91ab73993f2698bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "-------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looked at 0/29 samples\n",
            "Looked at 8/29 samples\n",
            "Looked at 16/29 samples\n",
            "\n",
            "Train loss: 1.75584 | Test loss: 0.68462, Test acc: 4336659.38%\n",
            "\n",
            "Epoch: 1\n",
            "-------\n",
            "Looked at 0/29 samples\n",
            "Looked at 8/29 samples\n",
            "Looked at 16/29 samples\n",
            "\n",
            "Train loss: 1.67982 | Test loss: 0.68035, Test acc: 4016021.88%\n",
            "\n",
            "Epoch: 2\n",
            "-------\n",
            "Looked at 0/29 samples\n",
            "Looked at 8/29 samples\n",
            "Looked at 16/29 samples\n",
            "\n",
            "Train loss: 1.48195 | Test loss: 0.69632, Test acc: 3296003.12%\n",
            "\n",
            "Train time on cuda:0: 13.137 seconds\n"
          ]
        }
      ],
      "source": [
        "# Set the seed and start the timer\n",
        "torch.manual_seed(42)\n",
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "# Set the number of epochs (we'll keep this small for faster training times)\n",
        "epochs = 3\n",
        "\n",
        "# Create training and testing loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    ### Training\n",
        "    train_loss = 0\n",
        "    #loop = tqdm(train_dataloader)\n",
        "    # Add a loop to loop through training batches # muitos valores para descompactar\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "        #print(X.shape,y.shape)\n",
        "        model.train()\n",
        "        X = X.to(device)\n",
        "        y = y.squeeze(1).type(torch.long).to(device)\n",
        "        #y = y.squeeze(1).to(device, dtype=torch.long)\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)['out']\n",
        "        # print(f\"\"\"\n",
        "\n",
        "        # Formatos: X: {X.shape}\n",
        "        #           Y: {y.shape},\n",
        "        #           Y_pred: {y_pred.shape},\n",
        "        #           batch: {batch}\n",
        "\n",
        "        # \"\"\")\n",
        "\n",
        "        # 2. Calculate loss (per batch)\n",
        "        loss = criterion(y_pred, y)\n",
        "        train_loss += loss # accumulatively add up the loss per epoch\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print out how many samples have been seen\n",
        "        #if batch % 100 == 0:\n",
        "        print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "    ## Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
        "    #train_loss /= len(train_dataloader)\n",
        "\n",
        "    ### Testing\n",
        "    # Setup variables for accumulatively adding up loss and accuracy\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in test_dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.squeeze(1).type(torch.long).to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred = model(X)['out']\n",
        "\n",
        "            # 2. Calculate loss (accumatively)\n",
        "            test_loss += criterion(test_pred, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
        "        # Divide total test loss by length of test dataloader (per batch)\n",
        "        test_loss /= len(test_dataloader)\n",
        "\n",
        "        # Divide total accuracy by length of test dataloader (per batch)\n",
        "        test_acc /= len(test_dataloader)\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
        "\n",
        "# Calculate training time\n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,\n",
        "                                           end=train_time_end_on_cpu,\n",
        "                                           device=str(next(model.parameters()).device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtwALwYuLRMx"
      },
      "source": [
        "Usar 🇰* https://pytorch.org/vision/stable/models/generated/torchvision.models.segmentation.deeplabv3_resnet101.html#torchvision.models.segmentation.deeplabv3_resnet101"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcofNJg4y8q0"
      },
      "source": [
        "a codificacao do target esta errada e proporcao em que ele foi normalizada esta ruim\n",
        "dividir todos por 255?\n",
        "normalizar de 0 a 1\n",
        "deixar de 0 a 255?\n",
        "utilizar one-hot encode?? (abordagem multiclasse)(mapear os pixels nas classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUkkxRUty8Fu"
      },
      "outputs": [],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKlauQTtccEO"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAJ_zUvsTHlx"
      },
      "outputs": [],
      "source": [
        "nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fO_mtk1Ufmns"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Feeg3yHGGe5h",
        "DgyuakE5YJcp",
        "v1pmbGuZ26xr"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6f419e594619498c91ab73993f2698bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e0317fe613448bd9b1c5c9c5022bfad",
              "IPY_MODEL_4f0ed539cfea488db5a2c66da32452b2",
              "IPY_MODEL_94ef2584fe0f4ff7b69736e416de795b"
            ],
            "layout": "IPY_MODEL_dfd2b4d672b74214b5baf7eb77fa1c5a"
          }
        },
        "3e0317fe613448bd9b1c5c9c5022bfad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1046266cc38b402183ac86b231f57f57",
            "placeholder": "​",
            "style": "IPY_MODEL_8ee02b09a4b446fda631862be1c587db",
            "value": "100%"
          }
        },
        "4f0ed539cfea488db5a2c66da32452b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_384dce75651f4f838f5d08a20e4b5141",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2c9a328adeb4e979b188d07cba60f44",
            "value": 3
          }
        },
        "94ef2584fe0f4ff7b69736e416de795b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_679528e9b8a04961993157644bbfbb0e",
            "placeholder": "​",
            "style": "IPY_MODEL_9230ac41958e4fb9a462c05b776426bd",
            "value": " 3/3 [00:13&lt;00:00,  4.32s/it]"
          }
        },
        "dfd2b4d672b74214b5baf7eb77fa1c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1046266cc38b402183ac86b231f57f57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ee02b09a4b446fda631862be1c587db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "384dce75651f4f838f5d08a20e4b5141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2c9a328adeb4e979b188d07cba60f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "679528e9b8a04961993157644bbfbb0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9230ac41958e4fb9a462c05b776426bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}